% Although SBI is founded an inferred degree of reversibil
% Moreover once the backward integration is performed to $t=0$, 

% Rather, the modified backward problem is related to SBI, as they are both idealized in the integrable cases of KdV.
% , rapidly overtaking and outperforming the conventional approach in almost every respect.
% More specifically, the trial states generated by the modified algorithm resemble the target state 
% The cost-functional $\J_f$ evaluated at $t=t_f$ can also be appl error of the initial state
% We compare  we include the additional terms, obtained from the integrable/inviscid 

% The modified procedure generalizes the SBI approximation by introducing a trial state.
% When this trial state is zero, we perform SBI as described orginally beginning with only the target final state.
% More generally we can incorporate the dynamics of a trial state to approximate its deviation using the same concept.
% Whereas SBI provides a crude approximation by assuming the target state is entirely reversible, the new procedure makes this assumption after the dynamics of a trial state are accounted for.
% As the trial state approaches the target, the approximation is designed to improve.
% Consequently this procedure mimics Newton's Method, even though it does not explicitly minimize any apparent functional.
% The basis of this modification is fully realized in the optimal perturbation problem, where the target state admits a massive symmetry.
% When the initial energy is constrained, the gradient of an optimal state must be parallel to the optimal state itself.
% (Otherwise there is some projection we can follow to further refine the trial solution.)
% Thus the adjoint loop nearly resembles an identity operator, and its constituents being the forward and adjoint problems must be almost mutually inverse.
% This is the exact behavior our modification seeks to exploit.
% No immediate obstacles preclude us from generalizing its application to include arbitrary objective functionals, which would nominally elicit source terms in the adjoint.
% Delayed evaluation of any objective at the late time $t=t_f$ allows us to reinterpret the problem in a form compatible with our modification.


% The algorithm is found to be curtailed by the effects of diffusion and nonlinear advection.
% We introduce our modified algorithm which accomodates for advection by ignoring diffusion.

% root-finding and thereby reduce the number of for.

% Adjoint-looping offers an efficient route for computing this high-dimensional gradient.
% consequences of optimization:
% 1. small response diffusion
% 2. local extrema
% 3. linear scaling grad obj
% 4. choice of objective?


% % Such problems arize when we want to probe the context an observation.
% % For example \cite{Liu2008,Li2017} aim to probe the history of Earth's mantle using present-day observations of its temperature distribution.
% % They assume the evolution of this temperature profile is governed by the viscous convection equations which cannot be solved backwards in time.
% % Instead, an optimal initial condition is computed which, when evolved to present-day, minimizes the observed residual.



% \begin{itemize}
%     \item Inverse problems are important and they're all over the place and people use them to gain insight into things they can't directly observe. Often we have a mathematical framework that we think describes a system, but when we take an observation and try to reconcile it with that framework, we can test a hypothesis of what the process is that led to those emergent observations.
%     \item Sometimes you're dealing with a linear problem, and apparently diffusion problems are nonlinear, but that's aside the point. But let's talk about like a wave problem that's definitely linear: linear inverse problems can be inverted. You can build a linear matrix that fully describes the action of the system, invert it, and get out your initial answer. So yay linear problems.
%     \item What about nonlinear problems??? You're screwed. Get out. Here's a bunch of sources that say gtfo: ..... . They say you have to reinterpret it as a constrained optimization problem.
%     Noteworthy exceptions.
%     At some point in the future, Liam will read 5-10 of these papers and summarize this difficulty in one paragraph. Part of it is that we have no global information about the system, e.g., there's no eigenmodes / eigenfunctions. OR the eigenfunctions do not describe the nonlinear actions. Since we have no global information, you just have to pick an initial point, and that might lead you to a wrong answer. (how do we know it's wrong?)
%     \item Direct adjoint looping (DAL): everyone seems to use it for the nonlinear inverse problem, and also other optimization problems of a similar nature. Algorithms which use DAL have recently been improved, but those improvements don't apply to problems where the initial conditions are unconstrained \cite{Mannix2022-discrete}.
%     \item The people who have studied DAL for convection in the mantle achieved some improvement (several papers, each getting better by X metric). They used a preconditioner which we cannot use with spectral methods, it's specific to their numerical methods. They have demonstrated the effectiveness of simple backward integration (SBI). SBI provides the authors with a reasonable first guess which helps get around this `no global information' problem.
%     \item In this work we present a generalization of SBI. Input thesis. profit. Sko cats.
%     \item Here's how the paper is organized.
% \end{itemize}

% % inverse problems r important
% Inverse problems can be solved to draw indirect conclusions from direct observations.
% This allows us to probe regions of space and time which are inaccessible.
% For example, the internal structures of stars and planets can be characterized by oscillatory measurements of their surfaces.
% Transient information can often be ascertained from an analogous class of inverse problems.
% - History

% % Here we aim to characterize the evolution which precedes a given observation. ??

% % nonlinear inverse problems are a special case of PDE-optimization
% Whereas linear inversions can be performed directly, nonlinear inverse problems are more intricate.
% A common approach uses constrained optimization algorithms to minimize a cost functional $\mathcal{J}$.
% \cite{Liu2008,Li2017} implement this approach to infer the history of Earth's mantle.
% Given a present-day measurement $\overline{u}(t_f)$ of the mantle's temperature distribution, the inverse problem is solved by determining the corresponding initial state $\overline{u}(0)$, where $\overline{u}(t)$ is constrained by the viscous convection equations.
% Starting with an initial guess $u(0) \sim \overline{u}(0)$, the optimization algorithm is applied iteratively to minimize the cost functional $\mathcal{J} = \frac{1}{2} \langle (u(t_f) - \overline{u}(t_f))^2 \rangle$.



% % Introduce DAL, conventional method
% Direct adjoint looping (DAL) allows us to efficiently compute functional derivatives and this technique has been applied well beyond the scope of inverse problems.
% Nonlinear systems and their optimal states are of particular interest, as they allow us to characterize bifurcations and hysteresis.
% Recently, [mannix] successfully accelerated DAL by remapping constraints onto spherical manifolds.

% % this is the closest thing I can find to my inverse problem:
% \cite{Liu2008,Li2017} used DAL to infer the history of Earth's mantle.
% These investigations studied inverse problems which coupled the viscous convection equations with contemporary observations.
% Given an observation at time $t=T$ we recover the corresponding initial state at time $t=0<T$ by minimizing a cost function.



% \vspace{1cm}


% \textbf{Previous work (DAL and SBI):}

% \textbf{Optimization preconditioners and spherical manifold}

% \textbf{Thesis:} DAL is outperformed by solving a modified backwards problem. This modified backwards problem becomes ideal for Integrable systems.

% Summation indices are fixed $i=j=1$ for 1D problems involving scalars, whereas for 2D problems $i,j\in\{1,2\}$.
% We use the Dedalus AAA pseudospectral python framework to solve all PDEs (forward and backward problems) with a fixed timestep.
% Mixed implicit-explicit (IMEX) timstepping schemes are implemented to evaluate nonlinear terms explicitly in a $3/2$-dealiased grid space.

% At an arbitrary initial state $X_i(x_i, 0)$, $\L$ varies with respect to the fields $\chi_i(x_i, 0<t<t_f)$, $X_i(x_i, t_t)$, $X_i(x_i, 0<t<t_f)$, and $X_i(x_i, 0)$ as follows
% \end{description}

% are assumed to have their own individual variations ().

  % This is realized by performing gradient-based minimization of the functional}
  % H[X, \overline{X}] &= \frac{1}{2}\davg{|X(\vec{x}, T) - \overline{X}(\vec{x}, T)|^2} = \frac{1}{2}\davg{X'^2(\vec{x}, T)} \label{Hdef},
% Suppose we observe the state $\overline{X}(x_i, t_f)$ at time $t=t_f$.

% Given a target final state $\overline{X}(\vec{x}, T)$, our aim is to recover the associated initial condition $\overline{X}(\vec{x}, 0)$.

% \begin{itemize}
%     \item define the inner product/norm
%     \item notation: bars indicate target solution, primes denote the deviation, tildes are multipliers
%     \item We have some observation $\xbar(t_f)$ which is the ``final state''.
%     \item $\xbar(t)$ is assumed to obey a given nonlinear PDE.
%     \item We aim to recover $\xbar(t_0)$.
%     \item The generalized problem has a linear and a nonlinear part (and maybe also a diffusive part but maybe that's nonlinear wtf idk).
%     \item Define the cost functional $\mJ$. Functional is a math thing, don't worry about it. Takes a function and spits out a number. Might have to define based on submission journal. Cost is the error, it's the number that comes out of the functional. Nicer to call the ``cost'' the ``objective''.
%     \item Show the circle and ellipse plot here.
%     \item The minimum of $\mJ$ (or its root) is the solution to our inverse problem, which is $\xbar(t_0)$. Show that with math. This will hold whenever the PDE is one-to-one. Say what one-to-one means here or earlier. One-to-one means: a future state must be associated with a unique past state -- two past states cannot produce an identical future state. If the two future states get close to each other that means the problem is ill-conditioned.
%     \item Define the lagrangian so that we don't have to do it twice.
% \end{itemize}

% \section{Linear considerations}




% \newpage
% \section{kdv-burgers}
% We study the generalized KdV-Burgers system
% \begin{align}
%   \mathcal{F}[\overline{w}(x,t)] = \partial_t \overline{w} + \overline{w}\partial_x \overline{w} + A_0 \partial_x^3 \overline{w} - B_0 \partial_x^2 \overline{w} &= 0 \label{kdvb_gen}
% \end{align}
% in a 1D periodic domain $x\in[0,2\pi]$. 
% The coefficients $A_0$ and $B_0$ are given constants.
% Equations of this kind describe nonlinear dispersive wave phenomena, such as acoustic waves and shallow-water surface waves.
% Several exact solutions to \ref{kdvb_gen} have been (and continue to be) found.
% In particular, consider the integrable case of KdV where $B_0=0$.
% A soliton initialized as 
% \begin{align}
%   \overline{w}(x, 0) &= 3\cosh^{-2}\Big(\frac{1}{2\sqrt{A_0}}\Big (x - \pi)\Big) \label{solic}
%   \intertext{will remain stable, as its solution}
%   \overline{w}(x, t) &= 3\cosh^{-2}\Big(\frac{1}{2\sqrt{A_0}}\Big (x - \pi - t)\Big)
% \end{align}
% translates with constant velocity $\Delta x / \Delta t = 1$. 
% The soliton traverses the domain with period $2\pi$ such that $\overline{w}(x, t+2\pi) = \overline{w}(x, t)$.
% Most importantly for our purposes, this case is time reversible.
% % Substituting $-\overline{w} \to \overline{w}$ in \ref{kdvb_gen} reverses the sign of $C_0$, meaning $-\overline{w}$ evolves backward in $t$ precisely the same as how $+\overline{w}$ evolves forward in $t$.
% \subsection{Direct Inversion Attempts}
% The retrospective inverse problem applied to \ref{kdvb_gen} becomes trivial when $B_0=0$, as we can always solve this PDE backwards in time, from $t:t_f\to 0$, starting with the target final state $\overline{w}_f$ and arriving at the target initial state $\overline{w}_0$.
% This is illustrated with initial condition \ref{solic} in the left column of Figure \ref{directinv}, where in the lower panel, the target solution undergoes two cycles of periodic evolution.
% The panel above this is an analytic mirror image where the exact inversion is carried out.

% Increasing the diffusivity $B_0 > 0$ along with the duration $t_f$ makes the inverse problem exceedingly difficult as one might expect.
% For $B_0=10^{-3}$ we reinitialize \ref{solic} and solve until $t_f=4\pi$. This is illustrated in the bottom center and right panels, where diffusion inhibits the soliton's advection.
% Consequently, the wave traverses the domain approximately once over $t:0\to t_f$, rather than twice.
% Using this new target solution, we attempt the direct inversion using Simple Backward Integration (SBI) and Quasi-Reversibility (QRM), as shown in the center and right columns of Figure \ref{directinv}.
% Both are intended to deliver an initial guess for our trial state $w_0$.

% % The KdV system ($B_0=0$) is an ideal application of SBI and QRM.
% % Both solve the unmodified forward problem \ref{kdvb_gen} in reverse, yielding the target state $\overline{w}_0$.

% SBI is implemented in the top row of the center column.
% The effects of diffusion are compounded because we reverse the sign of $B_0$ on $t:t_f\to 0$.
% Though the inversion mimics the dynamics of the forward problem, its result at $t=0$ does not resemble that of the target state.

% In the right column we achieve some success using QRM.
% This strategy has us append \ref{kdvb_gen} with a small biharmonic term
% \begin{align}
%   \partial_t \overline{w} + \overline{w}\partial_x \overline{w} + A_0 \partial_x^3 \overline{w} - B_0 \big(\partial_x^2 + \varepsilon\partial_x^4) \overline{w} &= 0. \label{qrm_kdv}
% \end{align}
% \ref{qrm_kdv} is well-posed when solving backward in time (provided $\varepsilon>0$).
% The free parameter $\varepsilon$ should be made as small as possible without causing numerical instability.
% In this context, QRM is effectively a low-pass filter applied to the ill-posed primitive problem.
% Our choice of $\varepsilon$ is related to the lengscale threshold, beyond which, high-wavenumber modes are quenched.
% Too much quenching and we will impede the soliton's advection, as is clearly the case with SBI.
% We find $\varepsilon=10^{-2}$ to a safe choice in this context. 
% % This design choice allows us to play with the threshold of lengthscales.

% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=5in]{directinv.pdf}
%   \caption{Direct inversions of KdV/KdV-Burgers initialized with \ref{solic}. \textbf{(Left column)} the forward solution (bottom row) consists of a stable soliton which traverses the domain precisely twice. This computation is then performed in reverse (top row) using an identical timestepping scheme. This trivial case does not benefit from SBI or QRM. \textbf{(Center column)} in the forward problem (bottom row) we introduce diffusion ($B_0 = 10^{-2}$) to stunt the soliton's motion and make the direct inversion ill-posed. SBI handles this ill-posed term by reversing its sign.
%   As shown in the top row, the soliton's motion is indeed reversed, but it continues to slow down due to diffusion.
%   \textbf{(Right column)} we recycle the previous forward solution and use QRM to attempt the direct inversion. We remedy the ill-posed problem by appending a small biharmonic hyperdiffusion term, whose coefficient $\varepsilon=10^{-2}$. This allows the ill-posed diffusion operator to repopulate some small-scale modes that decayed away during the forward solve. This is necessary in order for the soliton to grow and accelerate back to its original position \ref{solic}. SBI is typically stable numerically, whereas QRM is not. 
%   However its efficacy is clear from the approximate inversion show in both rows of the right column.}
%   \label{directinv}
%   % \includegraphics[width=4.5in]{../1drev/LI1000/targetsim_elapse.png}
% \end{figure*}

% \subsection{Adjoint-Looping Optimization}

% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=4.5in]{ic1.png}
%   % \includegraphics[width=4.5in]{../1drev/LI1000/objs.png}
% \end{figure*}

% \clearpage
% \section{Ideal Integrable Objective $\tilde{\mathcal{J}}$}
% Ideally, we want $\frac{\delta\mathcal{J}}{\delta u(x,0)} = u'(x,0)$, in which case a single adjoint-loop tells us exactly how to update $u(x, 0)$ such that $u(x, 0) \to \overline{u}(x, 0)$. Given that $u$ and $\overline{u}$ obey the forward system \eq{kdvburgers}, the deviation obeys
% \begin{align}
%   \partial_t u' + u'\partial_x \overline{u} + \overline{u} \partial_x u' + u'\partial_xu' + b\partial_x^3 u' &= a\partial_x^2 u'. \label{devPDE}
%   \intertext{We multiply \eq{devPDE} by $u'$ and take the domain integral}
%   \langle \frac{1}{2} \partial_t u'^2 \rangle + \langle u'\partial_x (u' \overline{u})\rangle + \langle u'^2 \partial_x u' \rangle + \langle u' b\partial_x^3 u'\rangle &= \langle au'\partial_x^2 u'\rangle.
%   \intertext{The last two terms on the LHS above go away, as they both conserve $u'^2$}
%   \langle \frac{1}{2} \partial_t u'^2 \rangle + \langle u'\partial_x (u' \overline{u})\rangle &= \langle au'\partial_x^2 u'\rangle.
% \end{align}
% \begin{align}
%   \intertext{Integrating over $t:0\to T$ and rearranging gives us the ideal objective functional}
%   \tilde{\mathcal{J}} &\equiv \langle \frac{1}{2} u'^2(x, T) \rangle + \int_0^T\langle u'\partial_x (u' \overline{u}) - au'\partial_x^2 u'\rangle dt \\
%   &= \mathcal{J}_0 + \int_0^T\langle u'\partial_x (u' \overline{u}) - au'\partial_x^2 u'\rangle dt \\
%   &= \langle \frac{1}{2} u'^2(x, 0) \rangle.
% \end{align}
% $\tilde{\mathcal{J}}$ is ideal, because $\frac{\delta\tilde{\mathcal{J}}}{\delta u(x,0)} = u'(x,0)$.
% \vspace{-0.2cm}
% \begin{align}
% \intertext{Using $\tilde{\mathcal{J}}$, the adjoint problem becomes}
%   \partial_t{\mu} + {u}\partial_x {\mu} + b\partial_x^3 {\mu} + a\partial_x^2 {\mu} + \frac{\delta\mathcal{J}}{\delta u} &= 0 \\
%   \partial_t{\mu} + {u}\partial_x {\mu} + b\partial_x^3 {\mu} + a\partial_x^2 {\mu} - 2a\partial_x^2u' - u'\partial_x (u - u')&= 0
% \end{align}

% \noindent Now our adjoint problem is ill-posed! Let's consider the trivial/reversible case. Let $a = 0$, $b = 0.1$, $T=4\pi$
% \begin{align}
%   \tilde{\mathcal{J}} &= \mathcal{J}_0 + \int_0^T\langle u'\partial_x (u' \overline{u}) \rangle dt = \mathcal{J}_0 + \int_0^T\langle u'\partial_x (u' (u - u')) \rangle dt \\
%   \intertext{Gives the adjoint problem}
%   0 &= \partial_t{\mu} + {u}\partial_x {\mu} + b\partial_x^3 {\mu} - u'\partial_x (u - u')\label{adjAP}
%   \intertext{with the same initial condition as when $\mathcal{J} = \mathcal{J}_0$}
%   \mu(x, T) &= -u'(x, T).
%   \intertext{For $a = 0$, \eq{adjAP} is equivalent to \eq{devPDE} if we substitute $-u'\to\mu$. Thanks to the above adjoint initial condition, we can rewrite \eq{adjAP} using the substitution $u'(x,t) \to -\mu(x, t)$. }
%   0 &= \partial_t{\mu} + {u}\partial_x {\mu} + b\partial_x^3 {\mu} + \textcolor{blue}{\mu\partial_x (u + \mu)}
% \end{align}

% \noindent Appending the blue term above allows us to compute $\tilde{\mathcal{J}}$ along with its functional derivative $\frac{\delta\tilde{\mathcal{J}}}{\delta u(x,0)} = u'(x,0)$, but only when $a = 0$.

% \clearpage
% \section{Advective Adjoint Modification}
% \noindent Now let's consider the nontrivial case from section II: $a = 0.001$, $b = 0.1$, $T=4\pi$. We include the additional (blue) term from the previous section in the adjoint
% \begin{align}
%   0 &= \partial_t{\mu} + {u}\partial_x {\mu} + b\partial_x^3 {\mu} + a\partial_x^2 \mu + \textcolor{blue}{\mu\partial_x (u + \mu)} \label{approxADJ}
%   % \partial_t \vec{\mu} + \color{blue}{\vec{\mu}\cdot \grad(\vec{u} + \vec{\mu})} &=\\
%   % \partial_t \vec{\mu} + \color{blue}{u''\cdot \grad u''} - \vec{u}\cdot\grad\vec{u} &=\\
% \end{align}
% % \textbf{Claim:} for small deviations and diffusivities, $|u'|\ll 1$ and $aT \ll 1$, this modification accelerates convergence.\newline

% \noindent Using the advective adjoint modification, we plot the evolution of $u(x,0)$, where the initial guess at iteration 1 is again generated using Simple Backward Integration (SBI). Notice how the modified adjoint optimization trajectory directs the initial condition toward the target state with precision. The bump is not initially dampened, nor does it wrap around the domain as it did in the convectional case.

% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=4.5in]{ic2.png}
% \end{figure*}

% \noindent Next we plot the conventional objective $\mathcal{J}_0$ as a function of iteration, with and without the advective adjoint modification. Notice how the conventional trajectory minimizes $\mathcal{J}_0$ more rapidly at first, as expected! But clearly, $\mathcal{J}_0$'s path of steepest-descent (shown in green) is not the most direct route toward the extremum. The modified adjoint trajectory (shown in blue) causes $\mathcal{J}_0$ to decrease gradually at first, but as $u(x,0)\to\overline{u}(x,0)$, we observe a sharp plunge in $\mathcal{J}_0$ near iteration 1000.

% % For the modified adjoint technique, we must employ Gradient Descent, as justified in the following section. Notice that this modification triggers a rapid increase in $\mathcal{J}_0$, followed by incredibly rapid convergence. \newline

% % We can illustrate the underlying behavior by plotting the normed deviation of the initial condition, $\langle (u(x,0) - \overline{u}(x,0))^2 \rangle$, shown below


% \subsection{Direct Adjoint Looping}
%   \begin{gather}
%   \intertext{We define the multipliers $\mu_i(x_i, t)$ and $\pi(x_i,t)$, along with the Lagrangian}
%   \mathcal{L} \equiv \int_0^T \langle \mu_i \big( \partial_t u_i + u_j\partial_j u_i + \partial_i p - \nu\partial_j\partial_ju_i \big) + \pi \partial_iu_i \rangle dt + \mathcal{J}\\
%   \intertext{The forward system is recovered by equating $\frac{\delta\mathcal{L}}{\delta\mu_i} = 0$ and $\frac{\delta\mathcal{L}}{\delta\pi} = 0$. To derive the adjoint system for an arbitrary objective, $\frac{\delta\mathcal{L}}{\delta p} = 0 = -\partial_i \mu_i + \frac{\delta \mathcal{J}}{\delta p}$, the Lagrangian is expressed as}
%   % \mathcal{L} \equiv \int_0^T \langle \mu_i \big( \partial_t u_i + u_j\partial_j u_i - \partial_i p - \nu\partial_j\partial_ju_i \big) + \pi \partial_iu_i \rangle dt + \mathcal{J}\\
%   \mathcal{L} = \langle  u_i(x_i,T) \mu_i(x_i,T) - u_i(x_i,0) \mu_i(x_i,0)  \rangle + \int_0^T \langle -u_i \partial_t\mu_i + \mu_i u_j\partial_j u_i + \mu_i\partial_i p - \mu_i\nu\partial_j\partial_ju_i + \pi \partial_iu_i \rangle dt + \mathcal{J}\\
%   \intertext{so that the adjoint system}
%   \frac{\delta\mathcal{L}}{\delta u_i} = 0 = -\partial_t\mu_i + \mu_j \partial_i u_j - u_j\partial_j \mu_i  - \nu\partial_j\partial_j\mu_i -  \partial_i\pi + \frac{\delta\mathcal{J}}{\delta u_i}\\.
%   \intertext{The conventional objective}
%   \mathcal{J}_0 \equiv \langle\frac{1}{2} |u_i(x_i, T) - \bar{u}_i(x_i, T)|^2\rangle = \langle\frac{1}{2} |u_i'(x_i, T)|^2\rangle
%   \intertext{where $u'_i \equiv u_i - \bar{u}_i$. This gives the adjoint initial condition}
%   \frac{\delta\mathcal{L}}{\delta u_i(x_i,T)} = 0 = \mu_i(x_i, T) + u'_i(x_i,T).
%   \intertext{For inviscid flow, evolution of the deviation is given by}
%   \partial_t u'_i + \bar{u}_j\partial_ju'_i + u'_j\partial_j\bar{u}_i + u'_j\partial_ju'_i + \partial_i p' = 0 \qquad \text{and} \qquad \partial_iu'_i = 0 \\
%   \partial_t \langle \frac{1}{2} |u'_i|^2 \rangle + \langle u'_i\bar{u}_j\partial_ju'_i + u'_iu'_j\partial_j\bar{u}_i \rangle = 0.
%   \intertext{Integrating over $t:0\to T$ gives the ideal objective}
%   \mathcal{J}_0 \equiv \mathcal{J}_f + \int_0^T \langle u'_i\bar{u}_j\partial_ju'_i + u'_iu'_j\partial_j\bar{u}_i \rangle dt = \frac{1}{2} ||u'_i(x_i, 0)||^2
%   \intertext{This introduces an analogous source term in the adjoint}
%   \frac{\delta\mathcal{L}}{\delta u_i} = 0 = -\partial_t\mu_i + \mu_j \partial_i u_j - u_j\partial_j \mu_i  - \nu\partial_j\partial_j\mu_i -  \partial_i\pi \color{blue}{- \mu_j \partial_j(u_i + \mu_i) - \mu_j \partial_i (u_j + \mu_j)} \\
% \end{gather}

% \clearpage

% \section{analysis}

% \section{generalization/applications}
% \begin{itemize}
%   \item minimal seeds have a special relationship with this class of inverse problems. A normalized initial state $X$ is guaranteed to be to maximize the late-time energy $X^2(T)$, provided $d(X^2(T)) / d(X(0)) \propto X(0)$.
%   This is a symmetry belonging to the PDE constraint; a special case where the seed plays the role on an eigenvector, and its functional derivative is given by multiplication with some eigenvalue.
%   These quantities being parallel implies that the functional derivative operator satisfies the definition of a conformal map at this one particular point.
%   Therefore seeds composed of harmonic functions
%   The eigenvalue is an arbitrary quantity unless we specify the leading coefficient of our objective $1/2*<u(T)>^2$.
%   Recall that the optimization routine itself always independent of this coefficient due to the linear adjoint.
%   The symmetry of interest relates the extremum's forward solution with its corresponding adjoint.
%   It is incredibly straightforward to understand symmetry in the context of an advective operator.
%   Symmetry afforded by the viscous forces is much more elusive and frankly surprising.

%   In theory, the forward problem maximizes the energy gain by triggering a lengthscale cascade.
%   This lengthscale cascade must be reversed via the adjoint problem.
%   The adjoint problem is a linearization about the optimum state, which must undo the lengthscale cascade, thereby ensuring that the loop does not change the input (seed)'s angle.
%   The simplest case assumes energy lost to viscous forces is small everywhere in the domain, rendering the flow is approximately reversible.
%   In this case, the adjoint and forward problem are approximately indistinguishable as the forward map A->B, the backward map B->A.
%   More generally, suppose the forward problem's viscous stress has a simmilar structure to that dissipated when the problem is solved in reverse.
%   This is consistent with our lengthscale casade model.
%   A turbulent transition sends energy to the smallest scales whereupon it is disspated.
%   Conversely, the reverse problem rapidly dissipates energy initialized at small scales, thereby undoing the cascade.

%   In any case, the modification can be implemented readily in this case and it's much easier to understand: the inverse problem behaves like navier stokes near the extremum, why not include this terms to make the backwards problem account for the nonlinear effects of advection.
%   Our inverse problem had no such assurances regarding forward/backward symmetries.
%   The minimal seeds are guaranteed a certain degree of this symmetry, contributing to the basis on which the modified terms are introduced.

%   Also of interest are case which seek to maximize the cumulative energy $\int u^2 dt$.
%   Equivalently we could consider minimizing the same quantity $\int delta(t-T) \int u^2 dt dt$ which transforms the adjoint source term into an analogous initial condition $mu(T) = \int 2u dt$, in which case perhaps $\partial_t \mu$ behaves like the forward velocity. Either way this persistent mean flow is somehow reduced by toward the associated minimal seed, which still satisfies the eigenvalue problem.

%   Ok so the minimal seed is parallel to the adjoint's solution if you load in the bulk flow, or if you load in the end flow. What does this tell us about the optimal state?

%   Well suppose a single optimum is shared between the time-integrated problem and the late-time problem.
%   Unsteady optimal states should have different eigenvalues for the bulk flow/late

%   % Consider the trivial case of irrotational flow, where the energy dissipation is uniform in space.
%   % Unforced irrotational flows satisfy the minimal seed criteria, as their forward problems undergo a uniform energy loss,



%   In these cases, the gradient is obtained using the same adjoint formulation as we layed out about, but instead of initializing the adjoint with the late-stage deviation, we initialize with the final state belonging to the trial function.
%   Minimal seeds must also satisfy an analogous optimization problem, where we aim to minimize the initial perturbation's energy subject to an energy constraint at late time.
%   This formulation would be inconvenient in practice, but it highlights a symmetry

%   \item could be combined with PINNs to speculate physics where training data is not easily accessible (example? there are lots of initial conditions out there, which land in many regimes. good luck generating enough training data to build a completely robust framework. the method we present can be used, like SBI, to try and find the correct NBHD)
%   \item the modified algorithm can be applied to other nonlinear IVPs
%   \item we could apply this to other inverse problems, such as finding time-dependent source terms and boundary conditions.
%   \item as noted in KdV section, optimization routines admit this linear property.
% \end{itemize}
% The End.


% \begin{abstract}
%     Time-reversal of diffusive physical systems is central in meteorology, cosmology, and geoscience. Though information about the initial state is lost in many such systems, we can generally recover the sought-after initial condition by solving a PDE-constrained optimization problem. We employ gradient-based adjoint looping with a least-squares difference objective functional to study this technique. In the context of several initial value problems, we examine various objective functionals' dependence in the space of initial conditions. By using an objective functional composed of spatial derivatives, we can occasionally accelerate convergence by recovering the small-scale features of initial conditions which are elusive in highly diffusive systems.
% \end{abstract}

% \section{Problem Setup}
% \noindent Consider a target solution $\overline{u}(x, t)$ satisfying KdV-Burgers
% \begin{align}
%   \partial_t\overline{u} &+ \overline{u}\partial_x \overline{u} + b\partial_x^3 \overline{u} = a\partial_x^2 \overline{u} \label{kdvburgers}
%   \intertext{in the periodic domain $x\in[0, 2\pi)$ with $t:0\to T$. Our aim is to approximate $\overline{u}(x, 0)$ given $\overline{u}(x, T)$. We define $\overline{u}(x,t)$'s approximation $u(x,t)$ which also satisfies \eq{kdvburgers}. Let the deviation $u'(x,t)\equiv u(x,t) - \overline{u}(x,t)$. We define the multiplier $\mu$ with corresponding Lagrangian}
%   \mathcal{L} &= \int_0^T \Big\langle \mu\Big( \partial_t{u} + {u}\partial_x {u} + b\partial_x^3 {u} - a\partial_x^2 {u} \Big) \Big\rangle dt + \mathcal{J},
%   \intertext{where the operator $\langle f(x) \rangle \equiv \int_0^{2\pi} f(x) \, dx$ and $\mathcal{J}$ is an arbitrary objective functional which we seek to maximize or minimize.}
%   \intertext{The forward problem}
%   \frac{\delta\mathcal{J}}{\delta \mu} &= 0 \quad \longrightarrow \quad \partial_t{u} + {u}\partial_x {u} + b\partial_x^3 {u} - a\partial_x^2 {u} = 0
%   \intertext{is solved from $t:0\to T$. The adjoint problem includes a source term}
%   \frac{\delta\mathcal{J}}{\delta u} &= 0 \quad \longrightarrow \quad \partial_t{\mu} + {u}\partial_x {\mu} + b\partial_x^3 {\mu} + a\partial_x^2 {\mu} + \frac{\delta\mathcal{J}}{\delta u}= 0
%   \intertext{and is solved from $t:T\to 0$. The adjoint initial condition}
%   \frac{\delta\mathcal{J}}{\delta u(x, T)} &= 0 \quad \longrightarrow \quad \mu(x,T) + \frac{\delta\mathcal{J}}{\delta u(x, T)} = 0
%   \intertext{and $\mathcal{J}$'s functional derivative wrt the initial condition}
%   \frac{\delta\mathcal{J}}{\delta u(x,0)} &= -\mu(x, 0).
% \end{align}

% \section{2D Advective Adjoint Modification}

% % Next consider a target 2D incompressible flow $\vec{\overline{u}}(\vec{x},t)$, governed by
% % \begin{gather}
% %   \partial_t \vec{\overline{u}} + \vec{\overline{u}}\cdot\grad\vec{\overline{u}} - \grad \overline{p} = \nu\laplacian\vec{\overline{u}} \qquad \text{and} \qquad \grad\cdot\vec{\overline{u}} = 0 \label{2dns}.
% %   \intertext{Let the approximation $\vec{{u}}(\vec{x},t)$ obey \ref{2dns}. We define the multipliers $\vec{\mu}(\vec{x}, t)$ and $\pi(\vec{x},t)$, along with the Lagrangian}
% %   \mathcal{L} \equiv \int_0^T \langle \vec{\mu} \cdot \big( \partial_t \vec{u} + \vec{u}\cdot\grad\vec{u} - \grad \overline{p} - \nu\laplacian\vec{u} \big) + \pi\big( \grad\cdot\vec{u} \big)\rangle dt + \mathcal{J}
% %   \intertext{The forward system is recovered by equating $\frac{\delta\mathcal{L}}{\delta\vec{\mu}} = \vec{0}$ and $\frac{\delta\mathcal{L}}{\delta\pi} = 0$.}
% %   \intertext{We derive the adjoint system for an arbitrary objective}
% %   \intertext{The conventional objective}
% %   \mathcal{J}_0 \equiv \langle\frac{1}{2} |\vec{u}(\vec{x}, T) - \overline{\vec{u}}(\vec{x}, T)|^2\rangle
% % \end{gather}

% \noindent Consider a target 2D incompressible flow $\overline{u}_i(\vec{x},t)$, governed by
% \begin{gather}
%   \partial_t \overline{u}_i + \overline{u}_j\partial_j\overline{u}_i + \partial_i \overline{p} = \nu\partial_j\partial_j\overline{u}_i \qquad \text{and} \qquad \partial_i\overline{u}_i = 0 \label{2dns}.
%   \intertext{Let the approximation $u_i(\vec{x},t)$ obey \ref{2dns}. We define the multipliers $\mu_i(\vec{x}, t)$ and $\pi(\vec{x},t)$, along with the Lagrangian}
%   \mathcal{L} \equiv \int_0^T \langle \mu_i \big( \partial_t u_i + u_j\partial_j u_i + \partial_i p - \nu\partial_j\partial_ju_i \big) + \pi \partial_iu_i \rangle dt + \mathcal{J}\\
%   \intertext{The forward system is recovered by equating $\frac{\delta\mathcal{L}}{\delta\mu_i} = 0$ and $\frac{\delta\mathcal{L}}{\delta\pi} = 0$.}
%   \intertext{We derive the adjoint system for an arbitrary objective}
%   \frac{\delta\mathcal{L}}{\delta p} = 0 = -\partial_i \mu_i + \frac{\delta \mathcal{J}}{\delta p}\\
%   \intertext{The Lagrangian can be expressed as}
%   % \mathcal{L} \equiv \int_0^T \langle \mu_i \big( \partial_t u_i + u_j\partial_j u_i - \partial_i p - \nu\partial_j\partial_ju_i \big) + \pi \partial_iu_i \rangle dt + \mathcal{J}\\
%   \mathcal{L} = \langle  u_i(\vec{x},T) \mu_i(\vec{x},T) - u_i(\vec{x},0) \mu_i(\vec{x},0)  \rangle + \int_0^T \langle -u_i \partial_t\mu_i + \mu_i u_j\partial_j u_i + \mu_i\partial_i p - \mu_i\nu\partial_j\partial_ju_i + \pi \partial_iu_i \rangle dt + \mathcal{J}\\
%   \intertext{yielding}
%   \frac{\delta\mathcal{L}}{\delta u_i} = 0 = -\partial_t\mu_i + \mu_j \partial_i u_j - u_j\partial_j \mu_i  - \nu\partial_j\partial_j\mu_i -  \partial_i\pi + \frac{\delta\mathcal{J}}{\delta u_i}\\
%   \intertext{The conventional objective}
%   \mathcal{J}_0 \equiv \langle\frac{1}{2} |u_i(\vec{x}, T) - \overline{u}_i(\vec{x}, T)|^2\rangle = \langle\frac{1}{2} |u_i'(\vec{x}, T)|^2\rangle
%   \intertext{where $u'_i \equiv u_i - \overline{u}_i$. This gives the adjoint initial condition}
%   \frac{\delta\mathcal{L}}{\delta u_i(\vec{x},T)} = 0 = \mu_i(\vec{x}, T) + u'_i(\vec{x},T).
%   \intertext{For inviscid flow, evolution of the deviation is given by}
%   \partial_t u'_i + \overline{u}_j\partial_ju'_i + u'_j\partial_j\overline{u}_i + u'_j\partial_ju'_i + \partial_i p' = 0 \qquad \text{and} \qquad \partial_iu'_i = 0 \\
%   \partial_t \langle \frac{1}{2} |u'_i|^2 \rangle + \langle u'_i\overline{u}_j\partial_ju'_i + u'_iu'_j\partial_j\overline{u}_i \rangle = 0.
%   \intertext{Integrating over $t:0\to T$ gives the ideal objective}
%   \tilde{\mathcal{J}} \equiv \mathcal{J}_0 + \int_0^T \langle u'_i\overline{u}_j\partial_ju'_i + u'_iu'_j\partial_j\overline{u}_i \rangle dt = \langle\frac{1}{2} |u'_i(\vec{x}, 0)|^2 \rangle
%   \intertext{This introduces an analogous source term in the adjoint}
%   \frac{\delta\mathcal{L}}{\delta u_i} = 0 = -\partial_t\mu_i + \mu_j \partial_i u_j - u_j\partial_j \mu_i  - \nu\partial_j\partial_j\mu_i -  \partial_i\pi \color{blue}{- \mu_j \partial_j(u_i + \mu_i) - \mu_j \partial_i (u_j + \mu_j)} \\
% \end{gather}

% \clearpage

% % \section{Modification Analysis}
% % Without the ill-posed term, this adjoint new no longer corresponds to the ideal objective! Instead, \eq{approxADJ} corresponds to a different, unideal objective $\tilde{\mathcal{J}}_a$ which depends on $\mu$. \newline

% % \noindent This is bad! If we set $\mathcal{J} = \tilde{\mathcal{J}}_a$, the forward system changes! (Recall that the forward system is derived by equating $\frac{\delta\mathcal{L}}{\delta \mu} = 0$.) Now the forward system has a source term involving $\mu$, but we can't solve for $\mu$ without first completing the forward solve. Thus we can't compute $\tilde{\mathcal{J}}_a$ or its functional derivative! But there's nothing stopping us from using this new adjoint system \eq{approxADJ}... \newline

% % \noindent Summing \eq{approxADJ} and \eq{devPDE} allows us characterize the error of this procedure
% % \begin{gather}
% %   \partial_t (u' + \mu) + (u'+\mu)\partial_x u + u \partial_x (u'+\mu) + \big( \mu\partial_x\mu - u'\partial_xu' \big) + b\partial_x^3 (u' + \mu) = a\partial_x^2 (u' - \mu) \label{uprimeprimeog}\\
% %   \intertext{We define the error $u''\equiv u' + \mu$ and derive its energy evolution}
% %   \partial_t \langle\frac{1}{2}u''^2\rangle + \langle\frac{1}{2}u''^2\partial_x u\rangle - \langle \frac{1}{2}u''^2\partial_x (\mu - u')\rangle = \langle au''\partial_x^2 (u' - \mu) \rangle
% %   \intertext{Near initialization $t \nearrow T$, we have}
% %   \partial_t (u' + \mu) = a\partial_x^2(u' - \mu)\\
% %   \intertext{we make the ansatz}
% %   \partial_t u' = a\partial_x^2u' \qquad\text{and}\qquad \partial_t \mu = -a\partial_x^2\mu.\\
% %   \intertext{For a characteristic Fourier mode $-\mu(x,t), u'(x,t) \propto e^{ikx}$, we find}
% %   u''(x,t) \sim |u'(x,T)|\sinh ak^2(T - t).
% %   \intertext{This ansatz is dimensionally consistent and satisfies some necessary conditions}
% %   u''(x,T)=0,\quad \partial_tu''(x,T) \neq 0, \quad\text{and}\quad \partial_t\langle u''^2(x,T) \rangle = 0.\\
% %   u' + \mu = \varepsilon u'' \\
% %   \varepsilon\partial_t u'' = a\partial_x^2 (\varepsilon u'' - 2\mu) \\
% %   u'' = -2at\varepsilon^{-1}\partial_x^2 \mu
% % \end{gather}

% % \begin{align}
% %   \intertext{Suppose a characteristic error $||u''||$ is attained at timescale $\tau$ such that. Let $u'' \equiv u' + \mu$. Near initialization $t \nearrow T$, we have $\partial_t u'' = a\partial_x^2 (u' - \mu)$. Suppose a characteristic error $||u''||$ is attained at timescale $\tau$ such that}
% % \tau \sim \frac{L^2||u''||}{2a||u'||} &
% %   \intertext{where $L$ is the lengthscale associated with the deviation $u'$. The approximation goes sour with Multiplying by $u'' \sim L/\tau$, meaning}
% %   \tau \sim \sqrt{\frac{L^3}{2a||u'||}} &\\
% %   % \longrightarrow \quad \partial_t \langle\frac{1}{2}(u' + \mu)^2\rangle + \langle\frac{1}{2}(u' + \mu)^2\partial_x u\rangle - \langle \frac{1}{2}(\mu + u')^2\partial_x (\mu - u')\rangle &= \langle a(\mu + u')\partial_x^2 (u' - \mu) \rangle
% %   \intertext{For $u' \sim -\mu$ and $|u'|, |\mu| \ll 1$, only the RHS diffusive term is dominant. Thus this new procedure should work well for small diffusivities ($a \ll 1$) in close proximity to the target ($|u'| \ll 1$)}
% % \end{align}
%   %  &= \mathcal{J}_0 + \int_0^T \langle \mu \partial_x(\mu(u + \mu)) \rangle dt\\
%   % &= \mathcal{J}_0 - \int_0^T \langle (\mu(u + \mu))\partial_x\mu  \rangle dt\\
% % \section{Introduction}
% % Rayleigh-B\'enard convection plays a foundational role in astrophysical and geophysical settings.
% % % The resulting buoyancy-driven flows regulate heat transfer and generate large-scale vortices \cite{Couston}.
% % % Turbulent convection, which is associated with large Rayleigh numbers $\Ra$, is difficult to simulate.
% % % State of the art simulations performed by \cite{Zhu_2018} have reached $\Ra \sim 10^{14}$ but estimates for the sun's convective zone and earth's interior are $\Ra \sim 10^{16}-10^{20}$ and $\Ra \sim 10^{20}-10^{30}$ respectively \cite{Ossendrijver,Gubbins_2001}.

% % \section{Generalized Problem Setup}
% % We define a generalized solution state vector $\overline{X}$ and associated initial value problem
% % \begin{align}
% %   \mathcal{F}[\overline{X}] &\equiv \partial_t \overline{X} + L . \overline{X} + N[\overline{X}, \overline{X}] = 0. \label{genpde}
% %   \intertext{Given a target final state $\overline{X}(\vec{x}, T)$, our aim is to recover the associated initial condition $\overline{X}(\vec{x}, 0)$.
% %   This is realized by performing gradient-based minimization of the functional}
% %   H[X, \overline{X}] &= \frac{1}{2}\davg{|X(\vec{x}, T) - \overline{X}(\vec{x}, T)|^2} = \frac{1}{2}\davg{X'^2(\vec{x}, T)} \label{Hdef},
% % \end{align}
% % with respect to $X(\vec{x}, 0)$, where $X(\vec{x}, t)$ obeys (\ref{genpde}). The operator $\davg{\cdot}$ denotes the domain-integrated average and the quantity $X'(\vec{x}, t) \equiv X(\vec{x}, t) - \overline{X}(\vec{x}, t)$. For the remainder of this paper, symbols with overbars (such as $\overline{X}(\vec{x}, T)$) will be used to denote the target solution, while their corresponding unannotated symbols (such as $X(\vec{x}, T)$) play the role of optimization parameters.

% % The Lagrangian associated with our initial value problem is given by
% % \begin{align}
% %   \mathcal{L} &= \int_0^T \davg{\chi \cdot \mathcal{F}[X]} dt + H[X, \overline{X}].
% %   \intertext{Where $\chi(\vec{x}, t)$ is the Lagrangian Multiplier corresponding to $X(\vec{x}, t)$. Equating} \frac{\delta\L}{\delta X} &= 0 \quad \text{and} \quad \frac{\delta\L}{\delta \chi} = 0
% %   \intertext{yields the original and adjoint systems}
% %   \mathcal{F}[X] &= 0 \quad \text{and} \quad \mathcal{F}^{\dagger}[\chi, X] = 0 \intertext{respectively. At some particular guess $X^n(\vec{x}, 0)$, we compute the functional derivative}
% %   \frac{\delta\L}{\delta X^n(\vec{x}, 0)} &= -\chi^n(\vec{x}, 0)
% %   \intertext{to determine the direction of steepest descent. Naive gradient-descent can then be performed by computing an updated guess}
% %   X^{n+1}(\vec{x}, 0) &= X^n(\vec{x}, 0) + \varepsilon \chi^n(\vec{x}, 0) \label{graddesc}
% %   \intertext{where $\varepsilon$ is the step size.}
% % \end{align}
% % In addition to this algorithm, more advanced gradient-based optimization techniques such as conjugate gradient and BFGS have been employed in conjunction with adjoint-looping to improve performance and accelerate convergence.
% % In the context of our time-inversion optimization problem, such techniques traverse the space of possible initial conditions $X(\vec{x}, 0)$ by relying on local information pertaining to the objective functional $H$ and its functional derivative $\frac{\delta H}{\delta X(\vec{x}, 0)}$.
% % For this reason, the focus of our investigation is largely centered around the objective functional's dependence on the initial condition.
% % More specifically, we study the levelsets of the objective functional because these surfaces characterize the directions of steepest descent at any arbitrary point in the space of initial conditions.
% % In doing so, we highlight the challenges and deficits associated with using the least squares functional (\ref{Hdef}) to recover the initial condition $\overline{X}(\vec{x}, 0)$ belonging to a target final state $\overline{X}(\vec{x}, T)$.


% % % Here I'll present a generalized PDE which we want to invert along with a bunch of notation. We'll define our target simulation, least-squares objective, and cite examples where the adjoint looping method has been successfully applied to inverse problems. We'll conclude this discussion by stressing the important of the objective function's shape in ``initial condition space'', because ultimately this is what curtails our gradient-based inversion technique.

% % \section{1-Dimensional Systems}
% % \subsection{Linearity and Reversiblity}
% % In this section, we study linear initial value problems which do not include the nonlinear term in (\ref{genpde}), i.e. $N[X, X] = N[\overline{X}, \overline{X}] = 0$. Therefore $\mathcal{F}[X'] = \mathcal{F}[X] - \mathcal{F}[\overline{X}]$.
% % In these special cases, the objective functional (\ref{Hdef}) depends only on the deviation's initial condition $X'(x, 0)$ about any arbitrary target $\overline{X}(x, 0)$.
% % % For convenience when dealing with linear problems, we take $U(0) = U(T) = 0$ for all $x$.
% % In the following section, we demonstrate that the objective functionals of nonlinear systems with nonzero targets are directly influenced by one's choice of $\overline{X}(x, 0)$.

% % Time-reversible wave equations of the form
% % \begin{align}
% %   \partial_t u &= \sum_{n \geq 1} c_n \partial^{2n - 1}_x u \label{waveeq}
% %   \intertext{are trivial cases of this optimization problem, because the quantity $\davg{u^2(x, t)}$ is conserved.
% %   This implies that the objective functional's levelsets form sphere's in $u'$-space, because its value is given by $\davg{u'^2(T)} = \davg{u'^2(0)}$.
% %   The spatial derivatives in (\ref{waveeq}) are anti-self-adjoint (i.e. $L[u] = -L^{\dagger}[u]$), so the path of steepest descent is always in the radial direction}
% %   \frac{\delta H}{\delta u(x, 0)} &= -u'(x, 0) \nonumber
% %   \intertext{In contrast, diffusive equations of the form}
% %   \partial_t u &= \sum_{n \geq 1} b_n \partial^{2n}_x u \label{waveeq}
% %   \intertext{do not conserve $\davg{u^2(x, t)}$, and consequently their objective functionals depend on $u'(x, 0)$'s \textit{direction} along with its magnitude. Suppose initial deviation can be written as}
% %   u'(x, 0) &= \sum_{k \geq 0} a_k e^{ikx}.
% % \end{align}

% % \begin{align}
% %   \intertext{Let's just consider the diffusion equation ($n=1$) with diffusivity $b_1$. The final state is given by}
% %   u'(x, T) &= \sum_{k\geq 0} a_k \exp(ikx-Tb_1k^2) \label{diffsoln}\\
% %   \intertext{and the nominal objective functional is given by}
% %   \davg{|u'(x, T)|^2} &= \pi \sum_{k \geq 0} \Big[a_k^2\exp\Big({-2T\sum_{n \geq 1} b_n k^{2n}  }\Big)\, \Big] \label{nomobjdiff}.
% %   \intertext{Equation (\ref{nomobjdiff}) gives us the elliptical heatmaps we observed previously, as shown in Figure \ref{diff_ellipse}.}
% % \end{align}

% % % \begin{figure}[h]
% % %   \includegraphics[width=9cm]{objtest_a0p03b0p0c0p0T3p0R0p015kt10p0kt20.png}
% % %   \caption{Objective function and an example gradient descent path for the diffusion equation. The target simulation $U(t) = 0$ and the deviation is composed of two fourier modes, i.e. $u'(t) = a_1\sin x + a_2\sin 2x$. Because the system is linear self-adjoint, each subsequent guess in the gradient descent trajectory is constrained to lie in this 2D phase space.}
% % %   \label{diff_ellipse}
% % % \end{figure}

% % \begin{align}
% %   \intertext{We could also consider an objective functional involving the final state's spatial derivative $\partial_xu'(x, T)$ like so}
% %   \davg{|\partial_x u'(x, T)|^2} &= \pi \sum_{k \geq 0} \Big[a_k^2 \; k^2 \exp\Big({-2T\sum_{n \geq 1} b_n k^{2n}  }\Big)\, \Big] \label{deriv1}.
% %   \intertext{Taking another derivative gives}
% %   \davg{|\partial_x^2 u'(x, T)|^2} &= \pi \sum_{k \geq 0} \Big[a_k^2 \; k^4 \exp\Big({-2T\sum_{n \geq 1} b_n k^{2n}  }\Big)\, \Big] \label{derivative2}.
% % \end{align}

% % \begin{align}
% %   \intertext{If we consider the second-order spatial derivative of (\ref{diffsoln})}
% %   \partial_x^2 u'(x, T) &= \sum_{k\geq 0} -k^2 a_k^2 \exp(ikx-Tb_1k^2)\\
% %   \intertext{In general, applying a spatial derivative of order $2m$ to (\ref{diffsoln}), yields}
% %   \partial_x^{2m} u'(x, T) &= \sum_{k\geq 0} (-k^2)^m a_k^2 \exp(ikx-Tb_1k^{2})\\
% %   \intertext{We can leverage this fact to construct an objective functional whose levelsets are spherical.}
% %   \intertext{Consider the following objective functional}
% % \end{align}
% % \vspace{-1cm}
% % \begin{tiny}
% % \begin{align*}
% %   H[u'(T)] &= \davg{|u'(x, T) - (Tb_1)\partial_x^2 u'(x, T) + \frac{(Tb_1)^2}{2!}\partial_x^4 u'(x, T) - \frac{(Tb_1)^3}{3!}\partial_x^6 u'(x, T) ...|^2} \\
% %   &= \davg{\Big|\sum_{k\geq 0}a_k \exp(ikx-Tb_1k^2) + \sum_{k\geq 0}(Tb_1k^2)a_k \exp(ikx-Tb_1k^2) + \sum_{k\geq 0}\frac{(Tb_1k^2)^2}{2!}a_k \exp(ikx-Tb_1k^2) + \sum_{k\geq 0}\frac{(Tb_1k^2)^3}{3!}a_k \exp(ikx-Tb_1k^2) + ...\Big|^2} \\
% %   &= \davg{|\sum_{k\geq 0}\Big[a_k \exp(ikx-Tb_1k^2)\Big(1 + (Tb_1k^2) + \frac{(Tb_1k^2)^2}{2!} + \frac{(Tb_1k^2)^3}{3!} ...\Big)\Big]|^2} \\
% %   &= \davg{|\sum_{k\geq 0}\Big[ a_k \exp(ikx-Tb_1k^2) \Big( \exp(Tb_1k^2) \Big)\Big]|^2} \\
% %   &= \davg{|\sum_{k\geq 0}\Big[ a_k \exp(ikx)  \Big)\Big]|^2} \\
% %   &= \pi \sum_{k\geq 0} a_k^2
% %   % &= \pi \sum_{k \geq 0} \Big[\exp\Big({-2T\sum_{n \geq 1} b_n k^{2n}  }\Big)\, a_k^2\Big].
% % \end{align*}
% % \end{tiny}
% % That gives us spherical objective levelsets. Notice what happens as we increment the number of terms in the above Taylor Series


\clearpage
% \section{Analysis of Methods}

% First let us define the Lagrangian corresponding to the formal adjoint of the viscous-Burgers problem
% \begin{align}
%   \L^{\dagger} &\equiv \int_0^{t_f} \langle u\, [\partial_t\mu + u\partial_x\mu + \nu \partial^2_x \mu] \rangle. 
%   \intertext{The adjoint system corresponding to $\L^{\dagger}$ is given by}
%   \frac{\delta \L^{\dagger}}{\delta \mu} = 0
%   &= -\partial_t u + \frac{\delta}{\delta \mu} \bigg\langle u^2 \partial_x \mu + \nu u\partial_x^2 \mu \bigg\rangle \\
%   &= -\partial_t u - 2u\partial_x u + \nu\partial_x^2 u.
%   \intertext{Thus, as one would anticipate, the adjoint of the adjoint is not the forward system. However, there is a different backward integration whose adjoint \textbf{is} the viscous burgers system.}
% \end{align}

% We define a new Lagrangian corresponding to SBI for the viscous-Burgers problem in 1D:
% \begin{align}
%   \L^{\rm{SBI}} &\equiv \int_0^{t_f} \langle u\, [\partial_t\mu + \mu\partial_xu + u\partial_x\mu + \mu\partial_x\mu + \nu \partial^2_x \mu] \rangle. \label{lsbi}
%   \intertext{Equating $\frac{\delta\L^{\rm{SBI}}}{\delta u}=0$ readily returns the desired perturbation equation for SBI. More enlightening is its corresponding adjoint}
%   \frac{\delta\L^{\rm{SBI}}}{\delta \mu} &= 0 = \partial_t u + u\partial_x u + \mu\partial_x u - \nu\partial_x^2 u.
% \end{align}

% We define a new Lagrangian corresponding to SBI for the viscous-Burgers problem in 1D:
% \begin{align}
%   \L^{\rm{SBI}} &\equiv \int_0^{t_f} \langle u\, [\partial_t\mu + \mu\partial_xu + u\partial_x\mu + \nu \partial^2_x \mu] \rangle. \label{lsbi}\\
%   &\equiv \int_0^{t_f} \langle u, F[u,\mu] \rangle dt. \label{lsbi}\\
%   \intertext{Equating $\frac{\delta\L^{\rm{SBI}}}{\delta u}=0$ readily returns} 
%   \partial_t\mu + \mu\partial_x u - \partial_x[u\mu] + 2u\partial_x\mu + \nu\partial_x\mu &= 0 
%   \intertext{the desired perturbation equation for SBI. More enlightening is its corresponding adjoint}
%   \frac{\delta\L^{\rm{SBI}}}{\delta \mu} &= 0 = \partial_t u + u\partial_x u + \mu\partial_x u - \nu\partial_x^2 u.
% \end{align}
% Notice how the backwards advection $\mu\partial_x\mu$ introduces a coupling term $\mu\partial_x u$. 
% Crucially, we observe that this coupling term alters what would otherwise become the forward problem.
% More precisely, as $u \to \overline{u}$, the coupling term vanishes.

% \begin{figure*}[h]
%   \centering
%   \includegraphics[width=4.5in, angle=0]{diagram.pdf}
%   \caption{Adjoint looping procedure for the retrospective inverse problem of finding $\overline{X}_i(x_i, 0)$ given $\overline{X}_i(x_i, t_f)$. A trial state $X_i(x_i, 0)$ is evolved to $X_i(x_i, t_f)$ which subsequently allows us to initialize the adjoint system via the compatibility condition. The adjoint system is evolved backward in time to $t=0$, where its solution allows us to update our trial state.}
%   \label{adj_diag}
%   % \includegraphics[width=4.5in]{../1drev/LI1000/targetsim_elapse.png}
% \end{figure*}
% \clearpage
% % \section{Analysis of Methods}

% This is a form of asymmetry. Namely, there are two Lagrangians corresponding to our forward problem and the linearization of SBI is like a dual-adjoint. 
% % Just not the adjoint readily corresponding to the inverse problem.
% The same holds for the optimal perturbation problem, as its adjoint is identical. 
% i.e. there is still an intrinsic backward integration whose adjoint gives the forward system in an identical fashion.
% We have not been able to demonstrate whether or not these ``dual-adjoints'' correspond to any 
% In that case, the auxiliary system still corresponds to the LSBI/perturbation equation about a hypothetical target state, but perhaps it is also related to an analogous optimization problem.
% The asymmetry may be related to the application of the constraint (energy budget at the final time rather than initial) and the objective (minimization of the initial energy rather than maximization of the final).


% \clearpage

% \section*{Appendix}
% We denote the target state vector $\overline{X}_i(x_i, t)$ where $0<t<t_f$.
% The retrospective inverse problem consists of reconstructing an initial condition, provided its solution is known at a given final time $t_f>0$.

% % We continue by itemizing several definitions and procedures as follows:
% \subsection{\textbf{Spatial Domain $x_i \in \mathcal{D}$}}
% Let the domain $\mathcal{D}$ be parameterized by the coordinate $x_i$ in the dimension corresponding to the index $i$.
% We assume $\mathcal{D}$ is periodic and bounded in each of these dimensions.
% Let the operation $\langle \cdot \rangle$ denote a spatial integral over this domain.
% Periodic boundary conditions do not allow flux into or out of the domain, meaning the divergence theorem applied to some quantity $Y_i$ reduces to $\langle \partial_i Y_i \rangle = 0$.

% \subsection{\textbf{Generalized Forward Problem}}
% We say the target state vector $\overline{X}_i(x_i, t)$ obeys an initial value problem of the form
% \begin{align}
%   \mathcal{F}[\overline{X}_i] &\equiv M.D_t \overline{X}_i + L . \overline{X}_i = 0, \label{genpde}
% \end{align}
% where the linear operators $M$ and $L$ have rank two. 
% We define the material derivative $D_t \overline{X}_i \equiv \partial_t \overline{X}_i + \overline{X}_j \partial_j \overline{X}_i$ where $\partial_j$ is the divergence tensor also having rank two.

% % \item{\textbf{Cost Functional and Lagrangian}}
% \subsection{\textbf{Trial Solutions $X_i$ and Deviations $X_i'$}}
% We are tasked with finding the target initial condition $\overline{X}_i(x_i, 0)$ belonging to a given final state $\overline{X}_i(x_i, t_f)$.
% We address this by constructing trial solutions $X_i(x_i, t)$ which are meant to approximate the target solution $\overline{X}_i(x_i, t)$ while satisfying \ref{genpde}.
% Each trial solution has some unknown deviation $X'_i(x_i, t) \equiv X_i(x_i, t) - \overline{X}_i(x_i, t)$.

% \subsection{\textbf{Cost Functionals $\mathcal{J}$, $\mathcal{J}_f$, $\mathcal{J}_0$}}
% Let $\mathcal{J}$ denote an arbitrary cost functional to be minimized.
% More precisely, we quantify the trial state's error by norming its deviation $X'_i$ at the final time $t_f$ as well as the initial time $t=0$.
% Their respective cost functionals
% \begin{align}
%   \J_f \equiv \frac{1}{2} \big| \big| M.X'_i(x_i, t_f) \big|\big|^2 \qquad \text{and} \qquad  \J_0 \equiv \frac{1}{2} \big|\big| M. X'_i(x_i, 0) \big|\big|^2
% \end{align}
% have coincident global minima where the trial and target states coincide.
% Though the deviation $X'_i(x_i, 0)$ at $t=0$ is assumed to be unknown, $\J_0$ provides a useful diagnostic in our numerical experiments.

% \subsection{\textbf{Lagrangian $\mathcal{L}$}}
% We define the corresponding Lagrangian
% \begin{align}
%   \L &= \int_0^{t_f} \langle \chi_i \mathcal{F}[X_i] \rangle dt + \J_f = \int_0^{t_f} \langle \chi_i [D_t X_i + L . X_i] \rangle dt + \frac{1}{2} \big|\big| M.X'_i(x_i, t_f) \big|\big|^2 
% \end{align}
% where $\chi_i(x_i, t)$ is the Lagrangian multiplier corresponding to $X_i(x_i, t)$.

% \subsection{\textbf{Definitions}}

% % \begin{figure}[H]
% %   \centering
% %   \begin{tabular}{@{}c@{}}
% %       \includegraphics[height=2.5in]{pp1.PNG}
% %   \end{tabular}
% %   \hfill
% %   \begin{tabular}{@{}c@{}}
% %       \includegraphics[height=2.5in]{pp2.PNG}
% %   \end{tabular}
% %   \newline
% %   \begin{tabular}{@{}c@{}}
% %       \includegraphics[height=2.5in]{pp3.PNG}
% %   \end{tabular}
% %   \hfill
% %   \begin{tabular}{@{}c@{}}
% %       \includegraphics[height=2.5in]{pp4.PNG}
% %   \end{tabular}
% %   \caption{Richardson Arms Race model phase portraits and sample trajectories for various configurations of parameters (given in the upper right corner of each subfigure).
% %   We observe a globally stable fixed point in the first case, unstable saddle points in the two following nonsingular cases, and no equilibria in the singular (bottom right) case.}
% %   \label{fig:pplane_rich}
% % \end{figure}


% % \begin{align}
%   The variations of $\L$ must vanish at its global minimum $X_i=\overline{X}_i$.
%   At an arbitrary initial state $X_i(x_i, 0)$, we follow these vanishing variations at the specified time slices:
%   \subsubsection{\textbf{Forward System} $(0<t<t_f)$}
%    Using our trial initial condition, we solve the original PDE constraint forward in time until $t=t_f$.
%    \begin{equation}
%     \frac{\delta \L}{\delta \chi_i}(x_i, t) = M.\big(\partial_t X_i + X_j\partial_jX_i\big) + L . X_i = 0
%    \end{equation}
%   \subsubsection{\textbf{Compatibility Condition} $(t=t_f)$}
%   This allows us to initialize the adjoint system once the forward solve is completed.
%   \begin{equation}
%     \frac{\delta \L}{\delta X_i}(x_i, t_f) = \chi_i(x_i, t_f) + X'_i(x_i, t_f) = 0
%   \end{equation}
%   \subsubsection{\textbf{Adjoint System} $(t_f>t>0)$}
%    The linear adjoint is solved backward in time until $t=0$.
%    \begin{equation}
%     \frac{\delta \L}{\delta X_i}(x_i, t) = -M.\big(\partial_t \chi_i + X_j\partial_j \chi_i - \chi_j\partial_iX_j\big) + L^{\dagger} . \chi_i = 0
%    \end{equation}
%   \subsubsection{\textbf{Initial Condition Evolution} $(t=0)$} 
%   By completing the adjoint loop we obtain
%   \begin{equation}
%     \frac{\delta \L}{\delta X_i}(x_i, 0) = -\chi_i(x_i, 0).
%   \end{equation}
% % \end{align}
% This is effectively $\J_f$'s gradient at a particular point in the space of trial initial states $\{X_i(x_i,0)\}$. 
% Methods such as steepest-descent and higher-order quasinewton schemes can then be implemented to traverse this space in search of the target state.
% The procedure is illustrated in Figure \ref{adj_diag}.


% \begin{center}
  % \begin{tabularx}{0.8\textwidth} 
    %   { 
      %   | >{\centering\arraybackslash}X 
      %   | >{\centering\arraybackslash}X 
      %   | >{\centering\arraybackslash}X | }
      %  \hline
      %  \textbf{Item} & \textbf{1D} & \textbf{2D} \\
      %  \hline
      %  item 21  & item 22  & item 23  \\
      % \hline
      % \end{tabularx}
      % \end{center}
      







% Taken as a whole, 

% We note a few important observations relating to this optimization framework:
% \begin{itemize}
%   \item Multiplication by a scalar $\alpha\J$ has an identical effect on its gradient $\alpha\frac{\delta \J}{\delta X(x, 0)}$. This implies that the adjoint system is always linear.
%   \item Suppose the trial state has some deviation $X'(x, t) \equiv X(x, t) - \overline{X}(x, t)$ which is known only at $t=t_f$.
%   This deviation has a magnitude $||X'(x, t_f)||$ and a normalized direction $X'(x, t_f)/||X'(x, t_f)||$.
%   % Provided this magnitude is finite, the linear adjoint 
%   The linear adjoint acts independently of this magnitude, yielding a gradient $\frac{\delta\L}{X(x, 0)}$ whose direction depends exclusively on the direction of the deviation.
%   % However the direction of $\J_f$'s gradient does not depend on $X'(x, t_f)$'s magnitude.
%   This becomes a critical limitation in the context of advective systems, where the velocity's magnitude and direction are inherently coupled.
%   \item Although it is generally effective, $\J_f$ is not the only plausible choice of cost functional.
%   More generally we can minimize a functional $\J$ with the same global minimum $\overline{X}(x, t_f)$.
%   For instance, any Sobolev norm
%   \begin{align}
%     \J_{k,p} &:= \frac{1}{2} || X(x, t_f) - \overline{X}(x, t_f)(x) ||^2_{k, p} \equiv \frac{1}{2}\Bigg( \sum_{j=0}^k \int_{\mathcal{D}} |\partial^j_{i} X(x, t_f) - \partial^j_{i} \overline{X}(x, t_f)(x)|^p dx \Bigg)^{2/p}
%   \end{align}
%   of integer $k$ and order $2\leq p < \infty$ would theoretically offer a feasible alternative.
%   \item The cost functional does not necessarily reflect how well our trial initial condition $X(x, 0)$ resembles the target $\overline{X}(x, 0)$.
%   One particular concern are local minima, consisting of suboptimal trial states ($X(x, 0)\neq \overline{X}(x, 0)$) which minimize $\J$ in the local sense.
%   Along those lines, ill-conditioning often manifests as weak dependence of the cost functional $\J$ on the trial initial condition $X(x, 0)$, such that $|| \frac{\delta \J}{\delta X(x, 0)} || \approx 0$.
%   For example, the diffusive system ($\mathcal{F}[X(x, t)] = \partial_t X - \nu\partial^2_{i} X = \vec{0}$) dampens the small-scale features of $X(x, 0)$ and $\overline{X}(x, 0)(x)$.
%   The previously-defined Sobolev functional combats this problem by incorporating derivatives.
%   \item Our choice of initial guess is of great importance for several obvious reasons. 
%   \cite{Liu2008,Li2017} demonstrate the advantages of a technique known as Simple Backward Integration (SBI), where a unique initial guess is constructed from the given final state $\overline{X}(x, t_f)(x)$. 
%   SBI is performed by integrating $\overline{X}(x, t_f)(x)$ backwards in time according to the forward problem, with the diffusive term reversed (otherwise the problem is ill-posed).
%   The underlying assumption is there must be a significant degree of reversibility in the forward problem's dynamics.
  
%   % \item 
%   % In particular, $\J_f$ might have local minima, a possibility which places greater emphasis on our choice of initial guess.
% \end{itemize} 



% Large data arrays are required to represent $X(x, 0)$ and its gradient $\frac{\delta\L}{\delta X(x, 0)}$.

% In we discretize so that $\frac{\delta\L}{\delta X(x, 0)}$ amounts to a high-dimensional gradient.
% This technique is especially useful when large resolutions are necessary to solve the forward problem.
% Parallel computation is often necessary to perform the forward and adjoint solves.
% Even when the space of possible initial conditions becomes tremendous, the adjoint-looping algorithm only consists of two solves.